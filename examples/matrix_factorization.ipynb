{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: matrix_factorization.html\n",
    "title: 'Training Example: Matrix Factorization'\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Install additional dependencies\n",
    "!pip install pytorch-lightning recsys_slates_dataset -q\n",
    "\n",
    "# If running in repo, append parent dir\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Define parameters for this run in a dictionary\n",
    "param = {\n",
    "    'dim' : 9,\n",
    "    'batch_size' : int(1e5),\n",
    "    'effective_batch_size' : int(2e6),\n",
    "    'sample_candidate_items' : 4, # If true, the dataloader adds an additional datapoint to each batch, \"allitem\", which is randomly sampled items to be used as negative feedback\n",
    "    'num_epochs': 100,\n",
    "    'overfit_batches' : False,\n",
    "    'name' : 'MatrixFactorization-CategoricalLoss'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from recsys_slates_dataset import lightning_helper\n",
    "dm = lightning_helper.SlateDataModule(num_workers=0, **param)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Model\n",
    "We implement a simple Matrix Factorization model using categorical losses (instead of the traditional Gaussian loss).\n",
    "Given a slate $S$ shown to the user $u$, the likelihood of clicking a specific item $c$ is:\n",
    "\n",
    "$$ \\frac{e^{z_u *v_c}}{\\sum_{i \\in S} e^{z_u *v_c}} $$ \n",
    "\n",
    "where \n",
    "$z_u$ is a parameter vector for user $u$,  \n",
    "$v_i$ is a parameter vector for item $i$,  \n",
    "and $x*y$ is the inner product between $x$ and $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "class SimilarityDot(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, Z, V):\n",
    "        return (Z * V).sum(-1)\n",
    "\n",
    "def dict_chunker(dict_of_seqs, size):\n",
    "    \"Iterates over the first dimension of a dict of sequences\"\n",
    "    length = len(dict_of_seqs[list(dict_of_seqs.keys())[0]]) # length of first idex\n",
    "    return ( {key : seq[pos:pos + size] for key, seq in dict_of_seqs.items()} for pos in range(0, length, size))\n",
    "\n",
    "class MatrixFactorization(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_users, \n",
    "        num_items,\n",
    "        dim=2, \n",
    "        lr_start=1e-3,\n",
    "        optim=\"adam\",\n",
    "        *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.score_func = SimilarityDot()\n",
    "        \n",
    "        # Initialize parameters\n",
    "        torch.manual_seed(1)\n",
    "        self.itemvec = nn.Embedding(self.hparams.num_items, self.hparams.dim)\n",
    "        nn.init.uniform_(self.itemvec.weight, a=-0.05, b=0.05)\n",
    "        self.uservec = nn.Embedding(self.hparams.num_users, self.hparams.dim)\n",
    "        nn.init.uniform_(self.uservec.weight, a=-0.05, b=0.05)\n",
    "\n",
    "    def loglik(self, batch):\n",
    "        # Get user and item parameters:\n",
    "        # Dimensions of tensors: [user/batch, interaction/step, item/slate, dim]\n",
    "        zetas = self.uservec(batch['userId']).unsqueeze(1).unsqueeze(1)\n",
    "        # Concatenate positive and negative items (first element is the positive one)\n",
    "        items = torch.cat((batch['click'].unsqueeze(-1), batch['allitem']), dim=-1)\n",
    "\n",
    "        # find the parameters vector corresponding to each item in the batch:\n",
    "        itemvecs_batch = self.itemvec(items)\n",
    "\n",
    "        # Compute the similarity (dot product) between the users and items for all items in all slates:\n",
    "        scores = self.score_func(zetas, itemvecs_batch)\n",
    "\n",
    "        # Set effectively zero probability for special Ids (0 is pad and 2 is UNK).\n",
    "        # These scores are log, so -100 is effectively 0: exp(-100)=4e-44\n",
    "        #scores[(batch['slate'] == 2) | (batch['slate'] == 0)] = -100\n",
    "\n",
    "        # Flatten all Tensors to [user, slatelength] (This simplifies the computation of the loss)\n",
    "        # We flatten by using a masking tensor that also selects the relevant data.\n",
    "\n",
    "        # Mask out data that are in a different phase AND datapoints that did not result in any clicks:\n",
    "        mask = (batch['phase_mask']*(batch['click']>=3)).bool()\n",
    "\n",
    "        scores_flat = scores[mask]\n",
    "\n",
    "        # Compute the \"allitem\" log likelihood of the observations:\n",
    "        # We use a categorical loss where all our positive signals are in the first dimension:\n",
    "        click_idx_flat = torch.zeros((scores_flat.size(0)), device=self.device)\n",
    "        loglik = dist.Categorical(logits=scores_flat).log_prob(click_idx_flat).sum()\n",
    "        return loglik\n",
    "    \n",
    "    #  TRAINING FUNCTIONS\n",
    "    def step(self, batch, batch_idx, phase):\n",
    "        stats = {}\n",
    "\n",
    "        stats['loglik'] = self.loglik(batch)\n",
    "        \n",
    "        # Since we are doing stochastic gradient decsent, \n",
    "        # multiply with the data factor to get estimate of the loss for the whole dataset:\n",
    "        data_factor = (self.hparams.num_users / batch['click'].size(0))\n",
    "        stats['loss'] = -(stats['loglik']*data_factor)\n",
    "\n",
    "        # Report loss and loglik:\n",
    "        with torch.no_grad():\n",
    "            for key, val in stats.items():\n",
    "                self.log(f\"{phase}/{key}\", val, on_step=False, on_epoch=True, sync_dist= (phase!=\"train\"))\n",
    "        \n",
    "        return stats['loss']\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Report mean absolute values of parameters:\n",
    "        for key, par in self.named_parameters():\n",
    "            self.log(f\"param/{key}-L1\", par.data.abs().mean(), on_step=False, sync_dist=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, phase=\"train\")\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, phase=\"valid\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        pars = self.parameters()\n",
    "        optimizer = torch.optim.AdamW(pars, lr=self.hparams.lr_start)\n",
    "        return optimizer\n",
    "\n",
    "    # PREDICT FUNCTIONS BELOW HERE\n",
    "    @torch.jit.export\n",
    "    def forward(\n",
    "        self, \n",
    "        batch : Dict[str, torch.Tensor], \n",
    "        targets: Optional[torch.Tensor]=None,\n",
    "        *args, **kwargs):\n",
    "        \"\"\" \n",
    "        Given a batch of data, estimate scores for all items in target.\n",
    "        If target is None, use all items.\n",
    "        NB: This function is very memory intensive, and need small batch sizes to avoid OOM.\n",
    "\n",
    "        Attributes:\n",
    "        batch: Dict[str, torch.Tensor] = {'userId' : torch.tensor([1,4,3])} # a batch of data of the same format as during training. For this matrix factorization model the only needed element is a tensor with userIds\n",
    "        targets: Optional[torch.Tensor] = None #  What ids should the model compute scores over. If None (default), all items in the model are evaluated.\n",
    "        \"\"\"\n",
    "\n",
    "        if targets is None:\n",
    "            targets = torch.arange(self.hparams.num_items,device=self.device)\n",
    "\n",
    "        target_vecs = self.itemvec(targets).unsqueeze(-2)\n",
    "        zetas = self.uservec(batch['userId']).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        scores = self.score_func(zetas,target_vecs).squeeze(-1)\n",
    "        return scores\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def recommend_batch(self, batch: Dict[str, torch.Tensor], num_rec=1, chunksize=3, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns the topk recommendations for all users supplied in batch.\n",
    "        The function does two things: It is a think topk-layer on top of self.forward, and it batches users so that the computer avoid OOM by trying to evaluate all users simulatenously. \n",
    "        For example, if chunksize=3 it will evaluate 3 and 3 users in a for loop.\n",
    "\n",
    "        Attributes:\n",
    "        batch: Dict[str, torch.Tensor] = {'userId' : torch.tensor([1,4,3])} # a batch of data of the same format as during training. For this matrix factorization model the only needed element is a tensor with userIds\n",
    "        targets: Optional[torch.Tensor] = None #  What ids should the model compute scores over. If None (default), all items in the model are evaluated.\n",
    "        \"\"\"\n",
    "        topk = torch.zeros((len(batch['click']), num_rec), device=self.device)\n",
    "\n",
    "        i = 0\n",
    "        for batch_chunk in dict_chunker(batch, chunksize):\n",
    "            pred = self.forward(batch=batch_chunk)\n",
    "            vals, topk_chunk = pred[:,3:].topk(num_rec, dim=1)\n",
    "            topk_chunk = 3+topk_chunk\n",
    "\n",
    "            topk[i:(i + len(pred))] = topk_chunk\n",
    "            i += len(pred)\n",
    "\n",
    "        return topk\n",
    "\n",
    "model = MatrixFactorization(num_items = dm.num_items, num_users = dm.num_users, **param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add callbacks\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"valid/loglik\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "cb = [\n",
    "    checkpoint_callback,\n",
    "    lightning_helper.Hitrate(dm, report_interval=100, num_rec=20),\n",
    "    lightning_helper.CallbackPrintRecommendedCategory(dm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    overfit_batches=param.get('overfit_batches', False), # for fast dry-runs\n",
    "    callbacks=cb,\n",
    "    logger = pl.loggers.TensorBoardLogger(f\"logs\", name=param['name']),\n",
    "    max_epochs=param['num_epochs'], \n",
    "    gpus= -1 if torch.cuda.is_available() else 0, \n",
    "    accumulate_grad_batches= int(param['effective_batch_size']/param['batch_size']), \n",
    "    weights_summary='full',\n",
    "    )\n",
    "\n",
    "#%% TRAIN\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "When training is done we can find the most relevant items for some users by using the forward function.\n",
    "The forward function will return a tensor of `[num_users_in_batch, num_targets]` with a score of each item per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch: Dict[str, torch.Tensor] = {'userId' : torch.tensor([1,4,3])}\n",
    "preds = model.forward(batch)\n",
    "print(preds.size())\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "name": "python3812jvsc74a57bd03c5be2b06ed6dbdf00174833742462bf4dfeb93002accc42ac9edec65c60a8dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
