{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lightning_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightning_helper\n",
    "\n",
    "> Helper functions for training models using the pytorch-lightning framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import recsys_slates_dataset.dataset_torch as dataset_torch\n",
    "import recsys_slates_dataset.data_helper as data_helper\n",
    "import pytorch_lightning as pl\n",
    "import logging\n",
    "class SlateDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    A LightningDataModule wrapper around the dataloaders created in dataset_torch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir= \"dat\",\n",
    "        batch_size=1024,\n",
    "        num_workers= 0,\n",
    "        sample_candidate_items=0,\n",
    "        valid_pct= 0.05,\n",
    "        test_pct= 0.05,\n",
    "        t_testsplit= 5, \n",
    "        limit_num_users=None,\n",
    "        *args, **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers =num_workers\n",
    "        self.sample_candidate_items=sample_candidate_items\n",
    "        self.valid_pct=valid_pct\n",
    "        self.test_pct=test_pct\n",
    "        self.t_testsplit=t_testsplit\n",
    "        self.limit_num_users = limit_num_users\n",
    "    def prepare_data(self):\n",
    "        \"\"\" \n",
    "        Download data to disk if not already downloaded.\n",
    "        \"\"\"\n",
    "        data_helper.download_data_files(data_dir=self.data_dir)\n",
    "\n",
    "    def setup(self, stage=None, num_negative_queries=0):\n",
    "\n",
    "        logging.info('Load data..')\n",
    "        self.ind2val, self.attributes, self.dataloaders = dataset_torch.load_dataloaders(\n",
    "            data_dir= self.data_dir,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers= self.num_workers,\n",
    "            sample_candidate_items=self.sample_candidate_items,\n",
    "            valid_pct= self.valid_pct,\n",
    "            test_pct= self.test_pct,\n",
    "            t_testsplit= self.t_testsplit,\n",
    "            limit_num_users=self.limit_num_users)\n",
    "\n",
    "        \n",
    "        # Add some descriptive stats to the dataset as variables for easy access later:\n",
    "        self.num_items = self.train_dataloader().dataset.data['slate'].max().item()+1\n",
    "        _ , self.num_interactions, self.maxlen_slate = self.train_dataloader().dataset.data['slate'].size()\n",
    "        self.num_users = self.train_dataloader().dataset.data['userId'].max().item()+1\n",
    "        self.num_interaction_types = len(self.ind2val['interaction_type'])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.dataloaders[\"train\"]\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.dataloaders[\"valid\"]\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.dataloaders[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "class CallbackPrintRecommendedCategory(pl.Callback):\n",
    "    \"\"\" A pytorch lightning callback that prints the clicks the user did, and the top recommendations at a given interaction.\"\"\"\n",
    "    def __init__(self, dm, num_recs=2, max_interactions=10, report_interval=100):\n",
    "        self.dm = dm\n",
    "        self.num_recs= num_recs\n",
    "        self.max_interactions=max_interactions\n",
    "        self.report_interval = report_interval\n",
    "\n",
    "        # Extract some data and index to report:\n",
    "        self.batch = next(iter(self.dm.train_dataloader())) # batch of data to visualize\n",
    "        self.idx = 12\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.report_interval==0:\n",
    "            for idx in [self.idx+k for k in range(5)]:\n",
    "                smallbatch = {key: val[idx].detach().clone().unsqueeze(0).to(pl_module.device).long() for key, val in self.batch.items()}\n",
    "\n",
    "                # Build recommendations for items:\n",
    "                M = torch.zeros(self.num_recs+1, self.max_interactions)\n",
    "                M[0,:] = smallbatch['click'].flatten()[:self.max_interactions] # add view to first row\n",
    "                for t_rec in range(self.max_interactions):\n",
    "                    scores = pl_module.forward_items(smallbatch, t_rec=t_rec)\n",
    "                    vals, rec_ids = scores.topk(self.num_recs)\n",
    "                    M[1:, t_rec] = rec_ids\n",
    "\n",
    "                def itemidx2string(itemidx):\n",
    "                    cat_idx = self.dm.attributes['category'][itemidx]\n",
    "                    s = self.dm.ind2val['category'][cat_idx]\n",
    "                    return s\n",
    "\n",
    "                title_mat = np.vectorize(itemidx2string)(M.long().numpy())\n",
    "\n",
    "                # compute the other elements:\n",
    "                slate_type = [self.dm.ind2val['interaction_type'][int(idx)] for idx in smallbatch['interaction_type'].flatten()]\n",
    "                row_tbl = lambda title,elements: f'| **{title}**   | {\" | \".join(elements[:self.max_interactions])} | '\n",
    "\n",
    "                table = []\n",
    "                table.append(f'| interaction step  | {\" | \".join([f\"t={i}\" for i in range(self.max_interactions)])} | ')\n",
    "                table.append(f'| -------           | {\"-------|\"*(self.max_interactions)}')\n",
    "                table.append( row_tbl(\"slate type\"   , slate_type) )\n",
    "                table.append( row_tbl(\"Clicks\", title_mat[0]) )\n",
    "                table.append(f'| -------           | {\"-------|\"*(self.max_interactions)}')\n",
    "                for k, elements in enumerate(title_mat[1:]):\n",
    "                    table.append( row_tbl(f\"rec item {k}\", elements) )\n",
    "\n",
    "                trainer.logger.experiment.add_text(f\"user_{idx}\", \"\\n \".join(table), global_step=trainer.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "class Hitrate(pl.Callback):\n",
    "    \"\"\" Module computing hitrate over the test dataset.\n",
    "    NB: This assumes that recommendations does not change over time. \n",
    "    I.e. will not work on temporal models.\n",
    "    \"\"\"\n",
    "    def __init__(self,dm, report_interval=100, num_rec=10, remove_already_clicked=True):\n",
    "        self.dm=dm\n",
    "        self.report_interval = report_interval\n",
    "        self.num_rec = num_rec\n",
    "        self.remove_already_clicked = remove_already_clicked\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calc_hits_in_batch(self, batch, pl_module):\n",
    "        # Move batch data to model device:\n",
    "        batch = {key: val.to(pl_module.device) for key, val in batch.items()}\n",
    "        \n",
    "        batch_recs = pl_module.recommend_batch(batch,num_rec= self.num_rec,t_rec=-1).detach().cpu()\n",
    "\n",
    "        # If a recommendation already appears in the training click sequence, remove it from recommendations.\n",
    "        # It is removed by setting the recommendation to a negative number ( :rolling_eyes:, i know), \n",
    "        # which will not be counted. This makes it faster&paralleizeable in the np.intersect1d part.\n",
    "        if self.remove_already_clicked:\n",
    "            dont_count_clicks = (batch['click']*(~batch['phase_mask'])).detach().cpu()\n",
    "            for n in range(batch_recs.size(1)):\n",
    "                rec_clicked_item = (batch_recs[:,n].unsqueeze(1)==dont_count_clicks).max(dim=1)[0]\n",
    "                batch_recs[rec_clicked_item,n] = -(1+n)\n",
    "\n",
    "        positive_clicks = (batch['click']*batch['phase_mask']).detach().cpu()\n",
    "\n",
    "        hits_in_batch = 0\n",
    "        for k in range(len(batch_recs)):\n",
    "            hits_in_batch += len(np.intersect1d(positive_clicks[k,], batch_recs[k,]))\n",
    "\n",
    "        num_users = batch_recs.size(0)\n",
    "        return hits_in_batch, num_users\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calc_hitrate(self, pl_module):\n",
    "        test_dataloader = self.dm.test_dataloader()\n",
    "        hits, users = 0,0\n",
    "        pbar = tqdm(test_dataloader, total=len(test_dataloader))\n",
    "        for batch in pbar:\n",
    "            pbar.set_description(f\"Hitrate Calc, hits/users: {hits}/{users}\")\n",
    "            hits_in_batch, num_users_batch = self.calc_hits_in_batch(batch, pl_module)\n",
    "            hits += hits_in_batch\n",
    "            users += num_users_batch\n",
    "        \n",
    "        hitrate = hits/users\n",
    "        return hitrate\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.report_interval==0:\n",
    "            hitrate = self.calc_hitrate(pl_module)\n",
    "            trainer.logger.experiment.add_scalar(f'test/hitrate_{self.num_rec}', hitrate, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 15:00:59,535 Downloading data.npz\n",
      "2022-02-07 15:00:59,536 Downloading ind2val.json\n",
      "2022-02-07 15:00:59,536 Downloading itemattr.npz\n",
      "2022-02-07 15:00:59,537 Done downloading all files.\n",
      "2022-02-07 15:00:59,538 Load data..\n",
      "2022-02-07 15:00:59,538 Download data if not in data folder..\n",
      "2022-02-07 15:00:59,539 Downloading data.npz\n",
      "2022-02-07 15:00:59,539 Downloading ind2val.json\n",
      "2022-02-07 15:00:59,540 Downloading itemattr.npz\n",
      "2022-02-07 15:00:59,541 Done downloading all files.\n",
      "2022-02-07 15:00:59,541 Load data..\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "dm = SlateDataModule()\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "checksum = next(iter(dm.train_dataloader()))['slate'].sum().item()\n",
    "assert checksum == 98897096275, \"Data error: Checksum of first batch is not expected value. Seed error?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
